{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENNI-B1 Morphosyntactic Analysis with Batchalign in Google Colab\n",
    "\n",
    "This notebook performs morphosyntactic analysis on ENNI-B1 CHAT files using batchalign with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up GPU and install dependencies\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install required packages\n",
    "!pip install batchalign torch numpy==1.24.0\n",
    "\n",
    "print('âœ“ Environment setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download ENNI-B1 files from GitHub\n",
    "def download_github_repo(repo_url, target_dir):\n",
    "    \"\"\"Download a GitHub repository as a zip file and extract it\"\"\"\n",
    "    # Create target directory\n",
    "    Path(target_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download the repository as zip\n",
    "    zip_url = repo_url.replace('github.com', 'github.com').replace('.git', '') + '/archive/refs/heads/master.zip'\n",
    "    \n",
    "    print(f\"Downloading {repo_url}...\")\n",
    "    response = requests.get(zip_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "            zip_ref.extractall(target_dir)\n",
    "        \n",
    "        print(f'âœ“ Downloaded and extracted to {target_dir}')\n",
    "        return True\n",
    "    else:\n",
    "        print(f'âœ— Failed to download: HTTP {response.status_code}')\n",
    "        return False\n",
    "    \n",
    "# Download your ICL-PILOT repository\n",
    "github_repo = 'https://github.com/your-username/ICL-PILOT.git'  # Replace with your actual GitHub repo\n",
    "data_dir = 'icl_pilot_data'\n",
    "\n",
    "if download_github_repo(github_repo, data_dir):\n",
    "    print('âœ“ Repository downloaded successfully')\n",
    "    \n",
    "    # Check what ENNI-B1 directories we have\n",
    "    enni_dirs = [\n",
    "        f'{data_dir}/ENNI_B1_TD',\n",
    "        f'{data_dir}/ENNI_B1_DLD',\n",
    "        f'{data_dir}/synthetic_data/ENNI_B1'\n",
    "    ]\n",
    "    \n",
    "    print('\\nENNI-B1 directories found:')\n",
    "    for enni_dir in enni_dirs:\n",
    "        if os.path.exists(enni_dir):\n",
    "            cha_count = len(list(Path(enni_dir).rglob('*.cha')))\n",
    "            print(f'  - {enni_dir}: {cha_count} .cha files')\n",
    "        else:\n",
    "            print(f'  - {enni_dir}: NOT FOUND')\n",
    "else:\n",
    "    print('âœ— Failed to download repository')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Set up output directories\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_base = 'analysis_results'\n",
    "utseg_output = f'{output_base}/utseg_results_{timestamp}'\n",
    "morphotag_output = f'{output_base}/morphotag_results_{timestamp}'\n",
    "\n",
    "os.makedirs(utseg_output, exist_ok=True)\n",
    "os.makedirs(morphotag_output, exist_ok=True)\n",
    "\n",
    "print(f'Output directories created:')\n",
    "print(f'  - Utseg: {utseg_output}')\n",
    "print(f'  - Morphotag: {morphotag_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run batchalign utseg (utterance segmentation)\n",
    "def run_batchalign_command(command, input_dir, output_dir, log_file):\n",
    "    \"\"\"Run a batchalign command with proper error handling\"\"\"\n",
    "    full_command = f'batchalign {command} {input_dir} {output_dir}'\n",
    "    \n",
    "    print(f'Running: {full_command}')\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            full_command.split(), \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=900  # 15 minutes timeout\n",
    "        )\n",
    "        \n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(f'Command: {full_command}\\n\\n')\n",
    "            f.write(f'Return code: {result.returncode}\\n\\n')\n",
    "            f.write('STDOUT:\\n')\n",
    "            f.write(result.stdout)\n",
    "            f.write('\\nSTDERR:\\n')\n",
    "            f.write(result.stderr)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f'âœ“ Successfully completed {command}')\n",
    "            return True\n",
    "        else:\n",
    "            print(f'âœ— Failed {command} (return code: {result.returncode})')\n",
    "            print(f'  See log: {log_file}')\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f'âœ— {command} timed out after 15 minutes')\n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(f'Command timed out: {full_command}')\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f'âœ— Exception running {command}: {str(e)}')\n",
    "        with open(log_file, 'w') as f:\n",
    "            f.write(f'Exception: {str(e)}')\n",
    "        return False\n",
    "    \n",
    "# Run utseg on each directory\n",
    "utseg_success = 0\n",
    "utseg_total = 0\n",
    "\n",
    "print('\\nStep 4: Running utseg (utterance segmentation)')\n",
    "print('=' * 60)\n",
    "\n",
    "for enni_dir in enni_dirs:\n",
    "    if os.path.exists(enni_dir):\n",
    "        utseg_total += 1\n",
    "        dir_name = os.path.basename(enni_dir)\n",
    "        dir_output = os.path.join(utseg_output, dir_name)\n",
    "        os.makedirs(dir_output, exist_ok=True)\n",
    "        \n",
    "        log_file = os.path.join(dir_output, f'{dir_name}_utseg.log')\n",
    "        \n",
    "        if run_batchalign_command('utseg', enni_dir, dir_output, log_file):\n",
    "            utseg_success += 1\n",
    "    else:\n",
    "        print(f'Failed on {enni_dir}')\n",
    "\n",
    "print(f'\\nUtseg results: {utseg_success}/{utseg_total} directories processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Run batchalign morphotag (morphosyntactic tagging)\n",
    "if utseg_success > 0:\n",
    "    print('\\nStep 5: Running morphotag (morphosyntactic tagging)')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    log_file = os.path.join(morphotag_output, 'morphotag.log')\n",
    "    \n",
    "    if run_batchalign_command('morphotag --retokenize', utseg_output, morphotag_output, log_file):\n",
    "        print('\\nâœ“ Morphosyntactic analysis complete!')\n",
    "    else:\n",
    "        print('\\nâœ— Morphotag failed')\n",
    "else:\n",
    "    print('\\nâœ— Skipping morphotag since utseg failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Generate summary and download results\n",
    "def generate_summary():\n",
    "    summary = f'ENNI-B1 Morphosyntactic Analysis Summary\n',\n",
    "    'Generated on: {datetime.now()}\n',\n",
    "'\\nEnvironment:\n',\n",
    "'  - Google Colab with GPU: {torch.cuda.is_available()}\n',\n",
    "'  - PyTorch version: {torch.__version__}\n',\n",
    "'  - NumPy version: {import numpy; numpy.__version__}\n',\n",
    "'\\nInput directories:\n'\n",
    "    \n",
    "    for enni_dir in enni_dirs:\n",
    "        if os.path.exists(enni_dir):\n",
    "            cha_count = len(list(Path(enni_dir).rglob('*.cha')))\n",
    "            summary += f'  - {enni_dir}: {cha_count} files\\n'\n",
    "    \n",
    "    summary += f'\\nProcessing results:\\n'\n",
    "    summary += f'  - Utseg: {utseg_success}/{utseg_total} directories\\n'\n",
    "    \n",
    "    if utseg_success > 0:\n",
    "        morphotag_status = 'Completed' if os.path.exists(morphotag_output) else 'Failed'\n",
    "        summary += f'  - Morphotag: {morphotag_status}\\n'\n",
    "    else:\n",
    "        summary += f'  - Morphotag: Skipped (utseg failed)\\n'\n",
    "    \n",
    "    summary += f'\\nOutput directories:\\n'\n",
    "    summary += f'  - Utseg results: {utseg_output}\\n'\n",
    "    summary += f'  - Morphotag results: {morphotag_output}\\n'\n",
    "    summary += f'\\nAll log files are available in the respective output directories.\\n'\n",
    "    \n",
    "    return summary\n",
    "    \n",
    "# Generate and display summary\n",
    "summary_content = generate_summary()\n",
    "print(summary_content)\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = f'{output_base}/analysis_summary_{timestamp}.txt'\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_content)\n",
    "\n",
    "print(f'\\nSummary saved to: {summary_file}')\n",
    "\n",
    "# Create a zip file of all results for easy download\n",
    "results_zip = f'{output_base}/enni_b1_analysis_results_{timestamp}.zip'\n",
    "\n",
    "print(f'\\nCreating results archive: {results_zip}')\n",
    "\n",
    "with zipfile.ZipFile(results_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(output_base):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, output_base)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f'âœ“ Results archive created: {results_zip}')\n",
    "\n",
    "# Provide download link\n",
    "from google.colab import files\n",
    "print(f'\\nðŸ“¥ Download your results:')\n",
    "files.download(results_zip)\n",
    "\n",
    "print(f'\\nðŸŽ‰ Analysis complete! You can also access all files in: {output_base}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Approach: Manual Analysis with spaCy\n",
    "\n",
    "If batchalign continues to have issues, here's an alternative using spaCy for morphosyntactic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy and download English model\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_with_spacy(cha_file, output_dir):\n",
    "    \"\"\"Analyze a CHAT file using spaCy\"\"\"\n",
    "    try:\n",
    "        with open(cha_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extract utterances (simplified - you'd need proper CHAT parsing)\n",
    "        doc = nlp(content)\n",
    "        \n",
    "        # Save analysis results\n",
    "        output_file = Path(output_dir) / f'{Path(cha_file).stem}_spacy_analysis.txt'\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'File: {cha_file}\\n')\n",
    "            f.write(f'Tokens: {len(doc)}\\n')\n",
    "            f.write(f'Sentences: {len(list(doc.sents))}\\n')\n",
    "            f.write('\\nMorphosyntactic Analysis:\\n')\n",
    "            \n",
    "            for token in doc[:50]:  # First 50 tokens as example\n",
    "                f.write(f'{token.text}\\t{token.pos_}\\t{token.tag_}\\t{token.dep_}\\n')\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'Error analyzing {cha_file}: {e}')\n",
    "        return False\n",
    "    \n",
    "# Example usage\n",
    "print('Testing spaCy analysis on one file...')\n",
    "test_file = next(Path('./ENNI_B1_TD').glob('*.cha')) if Path('./ENNI_B1_TD').exists() else None\n",
    "\n",
    "if test_file:\n",
    "    analyze_with_spacy(test_file, './spacy_analysis')\n",
    "    print(f'âœ“ spaCy analysis complete for {test_file}')\n",
    "else:\n",
    "    print('No test file found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "1. **GPU not detected**: Make sure you've selected GPU in Colab (Runtime > Change runtime type > GPU)\n",
    "\n",
    "2. **Batchalign hanging**: Try reducing the number of files or use the `--num_speakers` parameter\n",
    "\n",
    "3. **Memory issues**: Colab has limited memory. Process files in smaller batches.\n",
    "\n",
    "4. **Timeout errors**: Increase the timeout in the `run_batchalign_command` function.\n",
    "\n",
    "### Manual CHAT File Analysis:\n",
    "\n",
    "If automated tools fail, you can manually analyze CHAT files using:\n",
    "- **CLAN software** from TalkBank (http://talkbank.org/clan/)\n",
    "- **Custom Python scripts** using regular expressions to extract linguistic features\n",
    "- **Excel/CSV conversion** for manual coding and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}